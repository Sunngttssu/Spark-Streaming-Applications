{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTNfuVd5s84YVQnFYRRr+g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunngttssu/Spark-Streaming-BDA/blob/main/Word_count_using_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmBz61LEqmbC",
        "outputId": "ef49d845-eb05-454e-80e6-5cb69c6b89c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What: 1\n",
            "is: 5\n",
            "Spark: 9\n",
            "Apache: 1\n",
            "scalable: 1\n",
            "fault-tolerant: 1\n",
            "streaming: 6\n",
            "supports: 1\n",
            "both: 1\n",
            "an: 1\n",
            "of: 4\n",
            "engineers: 1\n",
            "scientists: 1\n",
            "process: 1\n",
            "real-time: 1\n",
            "(but: 1\n",
            "limited: 1\n",
            "Kinesis.: 1\n",
            "processed: 1\n",
            "pushed: 1\n",
            "out: 1\n",
            "systems,: 1\n",
            "live: 1\n",
            "dashboards.: 1\n",
            "Its: 1\n",
            "Stream: 1\n",
            "in: 1\n",
            "short,: 1\n",
            "into: 1\n",
            "batches.: 1\n",
            "DStreams: 1\n",
            "are: 1\n",
            "RDDs,: 1\n",
            "seamlessly: 1\n",
            "integrate: 1\n",
            "other: 3\n",
            "components: 1\n",
            "like: 1\n",
            "MLlib: 1\n",
            "different: 2\n",
            "systems: 1\n",
            "have: 2\n",
            "engine: 2\n",
            "designed: 1\n",
            "only: 1\n",
            "streaming,: 1\n",
            "similar: 1\n",
            "APIs: 1\n",
            "but: 1\n",
            "compile: 1\n",
            "engines.: 1\n",
            "single: 1\n",
            "execution: 1\n",
            "unified: 1\n",
            "programming: 1\n",
            "model: 1\n",
            "benefits: 1\n",
            "traditional: 1\n",
            "systems.: 1\n",
            ": 1\n",
            "Four: 1\n",
            "Major: 1\n",
            "stragglers: 1\n",
            "Better: 1\n",
            "balancing: 1\n",
            "resource: 1\n",
            "usage: 1\n",
            "Combining: 1\n",
            "static: 1\n",
            "datasets: 1\n",
            "integration: 1\n",
            "(SQL,: 1\n",
            "machine: 1\n",
            "learning,: 1\n",
            "graph: 1\n",
            "Streaming?: 1\n",
            "Streaming: 5\n",
            "a: 5\n",
            "processing: 3\n",
            "system: 1\n",
            "that: 3\n",
            "natively: 1\n",
            "batch: 3\n",
            "and: 11\n",
            "workloads.: 1\n",
            "extension: 1\n",
            "the: 1\n",
            "core: 2\n",
            "API: 1\n",
            "allows: 2\n",
            "data: 7\n",
            "to: 5\n",
            "from: 3\n",
            "various: 1\n",
            "sources: 1\n",
            "including: 1\n",
            "not: 1\n",
            "to): 1\n",
            "Kafka,: 1\n",
            "Flume,: 1\n",
            "Amazon: 1\n",
            "This: 2\n",
            "can: 1\n",
            "be: 1\n",
            "file: 1\n",
            "databases,: 1\n",
            "key: 1\n",
            "abstraction: 1\n",
            "Discretized: 1\n",
            "or,: 1\n",
            "DStream,: 1\n",
            "which: 1\n",
            "represents: 1\n",
            "stream: 1\n",
            "divided: 1\n",
            "small: 1\n",
            "built: 1\n",
            "on: 1\n",
            "Sparkâ€™s: 2\n",
            "abstraction.: 1\n",
            "with: 3\n",
            "any: 1\n",
            "SQL.: 1\n",
            "either: 1\n",
            "for: 2\n",
            "or: 1\n",
            "internally: 1\n",
            "lead: 1\n",
            "some: 1\n",
            "unique: 1\n",
            "over: 1\n",
            "Aspects: 1\n",
            "Fast: 1\n",
            "recovery: 1\n",
            "failures: 1\n",
            "load: 1\n",
            "interactive: 1\n",
            "queries: 1\n",
            "Native: 1\n",
            "advanced: 1\n",
            "libraries: 1\n",
            "processing): 1\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Specify the path to the input text file\n",
        "file_path = \"/content/word_count_example.txt\"\n",
        "\n",
        "# Read the file and proceed with the rest of the word count logic\n",
        "lines = sc.textFile(file_path)\n",
        "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "                   .map(lambda word: (word, 1)) \\\n",
        "                   .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "#Stop the SparkContext\n",
        "sc.stop()\n"
      ]
    }
  ]
}